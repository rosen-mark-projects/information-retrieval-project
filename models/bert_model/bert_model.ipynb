{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from feature_creator import create_features\n",
    "\n",
    "import tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(training, test):\n",
    "    training['keyword'] = training['keyword'].fillna('no_keyword')\n",
    "    test['keyword'] = test['keyword'].fillna('no_keyword')\n",
    "    training = training.fillna(0)\n",
    "    test = test.fillna(0)\n",
    "\n",
    "    dfs = create_features(training, test)\n",
    "    training = dfs[0]\n",
    "    test = dfs[1]\n",
    "\n",
    "    return [training, test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_tweets(tweets, tokenizer, max_length=512):\n",
    "    tokens = []\n",
    "    masks = []\n",
    "    segments = []\n",
    "    meta_features_count = 7\n",
    "\n",
    "    for index, row in tweets.iterrows():\n",
    "        tweet = tokenizer.tokenize(row.text)\n",
    "        keyword = tokenizer.tokenize(row.keyword)\n",
    "\n",
    "        tweet = tweet[:max_length - 2]\n",
    "        inputs = [\"[CLS]\"] + keyword + tweet + [\"[SEP]\"]\n",
    "        pad_length = max_length - len(inputs)\n",
    "\n",
    "        current_tokens = tokenizer.convert_tokens_to_ids(inputs)\n",
    "        current_tokens += [row.words, row.unique_words, row.stop_words, row.urls, row.mean_word_length, row.hashtags, row.mentionings]\n",
    "        pad_length -= meta_features_count # decrement with meta features count\n",
    "        current_tokens += [0] * pad_length\n",
    "        pad_masks = [1] * len(inputs) + [1] * meta_features_count + [0] * pad_length\n",
    "        segment_ids = [0] * max_length\n",
    "\n",
    "        tokens.append(current_tokens)\n",
    "        masks.append(pad_masks)\n",
    "        segments.append(segment_ids)\n",
    "\n",
    "    return np.array(tokens), np.array(masks), np.array(segments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bert_model(layer, max_length=512):\n",
    "    ids = Input(shape=(max_length,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    mask = Input(shape=(max_length,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segments = Input(shape=(max_length,), dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "    _, output = layer([ids, mask, segments])\n",
    "    clf_output = output[:, 0, :]\n",
    "    out = Dense(1, activation='sigmoid')(clf_output)\n",
    "\n",
    "    result_model = Model(inputs=[ids, mask, segments], outputs=out)\n",
    "    result_model.compile(Adam(lr=2e-6), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return result_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\n",
    "bert_layer = hub.KerasLayer(bert_url, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv(\"./../train.csv\")\n",
    "test_data = pd.read_csv(\"./../test.csv\")\n",
    "training_data = training_data[:10]\n",
    "test_data = test_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = process_data(training_data, test_data)\n",
    "training_data = processed_data[0]\n",
    "test_data = processed_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "lowercase = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "full_tokenizer = tokenization.FullTokenizer(vocabulary, lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_input = encode_tweets(training_data, full_tokenizer, max_length=160)\n",
    "test_input = encode_tweets(test_data, full_tokenizer, max_length=160)\n",
    "\n",
    "training_targets = training_data.target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_word_ids (InputLayer)     [(None, 160)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 160)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 160)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        [(None, 1024), (None 335141889   input_word_ids[0][0]             \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 1024)]       0           keras_layer[0][1]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            1025        tf_op_layer_strided_slice[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 335,142,914\n",
      "Trainable params: 335,142,913\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bert_model = create_bert_model(bert_layer, max_length=160)\n",
    "bert_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8 samples, validate on 2 samples\n",
      "Epoch 1/3\n",
      "8/8 [==============================] - 27s 3s/sample - loss: 0.4277 - accuracy: 1.0000 - val_loss: 0.4144 - val_accuracy: 1.0000\n",
      "Epoch 2/3\n",
      "8/8 [==============================] - 27s 3s/sample - loss: 0.3571 - accuracy: 1.0000 - val_loss: 0.3459 - val_accuracy: 1.0000\n",
      "Epoch 3/3\n",
      "8/8 [==============================] - 25s 3s/sample - loss: 0.2551 - accuracy: 1.0000 - val_loss: 0.2803 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "train_history = bert_model.fit(\n",
    "    training_input, training_targets,\n",
    "    validation_split=0.2,\n",
    "    epochs=3,\n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.44544512]\n",
      " [0.55637246]\n",
      " [0.6369516 ]\n",
      " [0.5019881 ]\n",
      " [0.49954873]\n",
      " [0.5707631 ]\n",
      " [0.4397103 ]\n",
      " [0.5814394 ]\n",
      " [0.4994784 ]\n",
      " [0.53205276]]\n"
     ]
    }
   ],
   "source": [
    "prediction = bert_model.predict(test_input)\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
